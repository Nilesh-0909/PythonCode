# -*- coding: utf-8 -*-
"""pyspark_pro.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g1jXBHOCKLdZWltElsJKdBTsNeapNAwQ
"""

from pyspark.sql import SparkSession
import os


spark = SparkSession.builder.appName("FoodDeliveryProject").getOrCreate()

df = spark.read \
    .format("csv") \
    .option("header", "true") \
    .option("inferSchema","true")\
    .load("/content/Zomato Dataset.csv")


df.show(5)
df.printSchema()

"""Remove Invalid Delivery_person_ID Records"""

from pyspark.sql import functions as F
from pyspark.sql.functions import col



def filter_valid_delivery_ids(df):
    """
    Remove null, blank, invalid, or malformed Delivery_person_ID values.
    """
    cleaned_df = df.filter(col("Delivery_person_ID").isNotNull() &
                       (col("Delivery_person_ID") != "") &
                       (col("Delivery_person_ID").rlike("^[A-Za-z0-9]+$")))

    return cleaned_df

clean_df = filter_valid_delivery_ids(df)
clean_df.show(5)

"""Use case 2 Clean invalid or out-of-range Delivery_person_Age."""

from pyspark.sql import functions as F
from pyspark.sql.functions import col


def filter_valid_delivery_age(df):
  # Filter for non-null ages and within the valid range of 18 to 70
  cleaned_df = df.filter(
      col("Delivery_person_Age").isNotNull() &
      (col("Delivery_person_Age") >= 18) &
      (col("Delivery_person_Age") <= 70)
  )
  return cleaned_df

Clean_age=filter_valid_delivery_age(df)
Clean_age.show(5)

"""Convert and clean Delivery_person_Ratings values."""

import pyspark.sql.functions as F
from pyspark.sql.functions import col, trim

def clean_delivery_ratings_one_step(df):
  df_clean = (
    df
    .withColumn("Delivery_person_Ratings", trim(col("Delivery_person_Ratings")))  # remove spaces
    .filter(col("Delivery_person_Ratings").rlike(r"^[0-9]+(\.[0-9]+)?$"))          # keep only numeric
    .withColumn("Delivery_person_Ratings", col("Delivery_person_Ratings").cast("float"))  # safe cast
    .filter((col("Delivery_person_Ratings") >= 1) & (col("Delivery_person_Ratings") <= 5))
)
  return df_clean # Added return statement

df_clean = clean_delivery_ratings_one_step(df)
df_clean.show()

def clean_rating(df):
  cleaned_rating=df.filter(col("Delivery_person_Ratings").isNotNull()&
                          (col("Delivery_person_Ratings")>=1)&
                          (col("Delivery_person_Ratings")<=5))
  return cleaned_rating

rating=clean_rating(df)
rating.show()

"""Convert Order_Date into proper date format.use case 4"""

from pyspark.sql import functions as F

def convert_date_in_proper_format(df):
    return df.withColumn(
        "Order_Date",
        F.to_date(
            F.trim(F.col("Order_Date")),   # remove spaces
            "dd-MM-yyyy"                   # exact format
        )
    )
#2022-02-12  yyyy-mm-dd this is the defualt format spark return ..

pro_date=convert_date_in_proper_format(df)
pro_date.show(5)

"""Same use case but output format is def,we assign the output format"""

from pyspark.sql import functions as F
from pyspark.sql.functions import to_timestamp, col,date_format,coalesce


def convert_date_in_proper_format(df):
    # Step 1: read the date string correctly
    df = df.withColumn(
        "Order_Date",
        F.to_date(F.trim(F.col("Order_Date")), "dd-MM-yyyy")
    )

    # Step 2: convert to your desired output format
    df = df.withColumn(
        "Order_Date",
        F.date_format(F.col("Order_Date"), "yyyy/MM/dd")   # desired output
    )

    return df
#to_date gives input format to spark and date_format conver the format you want as output

user_define_format=convert_date_in_proper_format(df)
user_define_format.show(5)

"""use case 5 Clean and parse Time_Orderd and Time_Order_picked.

"""

from pyspark.sql.functions import col, to_timestamp, date_format, coalesce

def convert_time_in_proper_format(df, cols):
    for c in cols:
        df = df.withColumn(
            c,
            date_format(
                coalesce(
                    to_timestamp(col(c), "hh:mm a"),
                    to_timestamp(col(c), "h:mm a"),
                    to_timestamp(col(c), "HH:mm"),
                    to_timestamp(col(c), "HH:mm:ss")
                ),
                "HH:mm:ss"
            )
        )
    return df
#Output type: timestamp (has both date + time)
# usage
df = convert_time_in_proper_format(df, ["Time_Orderd", "Time_Order_picked"])
df.show(5)

"""use case 6 Standardize text columns (trim, lower case)."""

# Get all columns with string type
string_cols = [c[0] for c in df.dtypes if c[1] == 'string']
print("All string columns:", string_cols)

from pyspark.sql.functions import col, trim, lower

def standardize_text_columns(df):

    # STEP 1: Get only string columns
    string_cols = [c for c, t in df.dtypes if t == "string"]

    text_cols = []

    # STEP 2: Detect columns that contain letters A-Z
    for c in string_cols:
        sample = df.select(c).filter(col(c).rlike("[A-Za-z]")).count()

        if sample > 0:
            text_cols.append(c)

    # STEP 3: Clean detected text columns (trim + lower)
    for c in text_cols:
        df = df.withColumn(c, lower(trim(col(c))))

    return df, text_cols

cleaned_df, text_cols = standardize_text_columns(df)
cleaned_df.show(5)

"""Same use case using list comp"""

def convert_String_col_into_lower(df):
    string_col=[i for i,j in df.dtypes if j =="string"]
    df=df.select([lower(col(i)).alias(i) if i in string_col else col(i) for i in df.columns ])
    return string_col,df

r=convert_String_col_into_lower(df)
print(r)

df.select("Delivery_person_Ratings").show(5)

"""Average delivery time per City."""

from pyspark.sql.functions import *

df_city_avg = df.groupBy("City").agg(
    avg("Time_taken (min)").alias("Time_taken (min)")
)
df_city_avg.show()

"""Average delivery time per Type_of_vehicle."""

from pyspark.sql.functions import *

df_city_avg = df.groupBy("Type_of_vehicle").agg(
    avg("Delivery_location_latitude").alias("Delivery_location_latitude_avg"),
    avg("Delivery_location_longitude").alias("Delivery_location_longitude_avg")
)
df_city_avg.show()

"""Find peak order hours from Time_Orderd."""

from pyspark.sql.functions import col, hour, to_timestamp, trim

def find_peak_order_hours(df):

    # 1️⃣ Clean the time column (convert to timestamp)
    df = df.withColumn(
        "Time_Orderd",
        to_timestamp(trim(col("Time_Orderd")), "HH:mm:ss") # Changed format to HH:mm:ss
    )

    # 2️⃣ Remove rows where time is NULL after conversion
    df = df.filter(col("Time_Orderd").isNotNull())

    # 3️⃣ Extract hour from cleaned timestamp
    df = df.withColumn("hour", hour(col("Time_Orderd")))

    # 4️⃣ Group by hour and count orders
    result = df.groupBy("hour").count().orderBy(col("count").desc())

    return result

hour=find_peak_order_hours(df)
hour.show()

"""Calculate delivery distance using lat-long"""

from pyspark.sql.functions import col, unix_timestamp, when

def calculate_pickup_delay(df):
    """
    This function:
    1. Converts Time_Orderd and Time_Order_picked to timestamp
    2. Calculates pickup delay in minutes
    3. Returns the enhanced DataFrame
    """

    df = (
        df
        # Step 1: Convert both time columns to timestamp
        .withColumn("Time_Orderd", col("Time_Orderd").cast("timestamp"))
        .withColumn("Time_Order_picked", col("Time_Order_picked").cast("timestamp"))

        # Step 2: Calculate pickup delay (in minutes)
        .withColumn(
            "pickup_delay_minutes",
            when(
                col("Time_Orderd").isNotNull() & col("Time_Order_picked").isNotNull(),
                (unix_timestamp(col("Time_Order_picked")) - unix_timestamp(col("Time_Orderd"))) / 60
            ).otherwise(None)
        )
    )

    return df

enhanced=calculate_pickup_delay(df)
enhanced.show(5)

"""Analyze impact of multiple_deliveries on time taken."""

from pyspark.sql.functions import col, when, avg, coalesce, lit

temp_df = enhanced

temp_df = temp_df.withColumn(
    "multiple_deliveries",
    when(
        col("multiple_deliveries").isNull() |
        (col("multiple_deliveries") == "") |
        (col("multiple_deliveries") == "NaN") |
        (col("multiple_deliveries") == "nan"),
        0
    ).otherwise(col("multiple_deliveries").cast("int"))
)

# Group by multiple_deliveries and calculate avg Time_taken
result_df = temp_df.groupBy("multiple_deliveries") \
              .agg(avg(col("Time_taken (min)")).alias("avg_time_taken"))


result_df.show()

"""
Use window functions to rank fastest delivery persons"""

from google.colab import drive
drive.mount('/content/drive')

from pyspark.sql import functions as F
from pyspark.sql.window import Window

def rank_fastest_delivery_persons(df):
    """
    Rank delivery persons based on their average delivery time using window function.
    Steps:
    1. Calculate avg Time_taken per delivery person.
    2. Apply window function to assign rank.
    3. Sort result by rank.
    """

    # Step 1: Average time taken per delivery person
    avg_df = df.groupBy("Delivery_person_ID") \
               .agg(F.avg("Time_taken (min)").alias("avg_time"))

    # Step 2: Define window for ranking
    window_spec = Window.orderBy(F.col("avg_time").asc())

    ranked_df = avg_df.withColumn(
    "rank",
    F.dense_rank().over(window_spec)

)
    return avg_df,window_spec,ranked_df

avg_df,window_spec,ranked_df=rank_fastest_delivery_persons(df)
ranked_df.show(5)

